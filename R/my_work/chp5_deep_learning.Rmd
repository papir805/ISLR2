---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
```

```{r}
lfit <- lm(Salary ~ ., data = Gitters[-testid, ])

lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
```

```{r}
x <- model.matrix(Salary ~ . - 1, data = Gitters)
x_scale <- scale(x)
y <- Gitters$Salary
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
```{r}
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
                   type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
```

```{r}
library(keras)
modnn <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = 'relu',
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1)
```

```{r}
x <- model.matrix(Salary ~ . -1, data = Gitters) %>% scale()

modnn %>% compile(loss = 'mse',
                  optimizer = optimizer_rmsprop(),
                  metrics = list("mean_absolute_error")
                  )
```

```{r}
history <- modnn %>% fit(
  x[-testid, ], y[-testid], epochs=1500, batch_size=32,
  validation_data = list(x[testid, ], y[testid])
)
```

```{r}
plot(history)
```

```{r}
npred <- predict(modnn, x[testid, ])
mean(abs(y[testid] - npred))
```

```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
g_train <- mnist$train$y
x_test <- mnist$test$x
g_test <- mnist$test$y
dim(x_train)

dim(x_test)
```

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
```

```{r}
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
```

```{r}
modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units=256, activation='relu', input_shape= c(784)) %>%
  layer_dropout(rate=0.4) %>%
  layer_dense(units=128, activation='relu') %>%
  layer_dropout(rate=0.3) %>%
  layer_dense(units=10, activation='softmax')
```

```{r}
summary(modelnn)
```

```{r}
modelnn %>% compile(loss = 'categorical_crossentropy',
                    optimizer = optimizer_rmsprop(), metrics = c('accuracy')
)
```

```{r}
system.time(
  history <- modelnn %>%
    fit(x_train, y_train, epochs = 30, batch_size = 128,
        validation_split = 0.2)
)

plot(history, smooth=FALSE)

```

```{r}
accuracy <- function(pred, truth)
  mean(drop(pred) == drop(truth))
modelnn %>% predict_classes(x_test) %>% accuracy(g_test)
```

```{r}
modellr <- keras_model_sequential() %>%
  layer_dense(input_shape = 784, units = 10,
              activation='softmax')
summary(modellr)
```

```{r}
modellr %>% compile(loss = 'categorical_crossentropy',
                    optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
modellr %>% fit(x_train, y_train, epochs = 30,
                batch_size = 128, validation_split = 0.2)
modellr %>% predict_classes(x_test) %>% accuracy(g_test)
```

```{r}
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
g_train <- cifar100$train$y
x_test <- cifar100$test$x
g_test <- cifar100$test$y

dim(x_train)

range(x_train[1,,, 1])
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(g_train, num_classes=100)
dim(y_train)
```

```{r}
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,, ]))
```
```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding= 'same', activation = 'relu',
                input_shape = c(32, 32, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3),
                padding = 'same', activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3),
                padding='same', activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 256, kernel_size = c(3,3),
                padding='same', activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = 'relu') %>%
  layer_dense(units = 100, activation = 'softmax')
summary(model)
```

```{r}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epochs = 30,
                         batch_size = 128, validation_split = 0.2)
model %>% predict_classes(x_test) %>% accuracy(g_test)
```

```{r}
img_dir <- '/Users/rancher/Google Drive/Coding/ISLR2/Python/chp10/labs/book_images'
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
  img_path <- paste(img_dir, image_names[i], sep='/')
  img <- image_load(img_path, target_size = c(224, 224))
  x[i,,, ] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
```

```{r}
model <- application_resnet50(weights = 'imagenet')
summary(model)
```

```{r}
pred6 <- model %>% predict(x) %>%
  imagenet_decode_predictions(top = 3)
names(pred6) <- image_names
print(pred6)
```

```{r}
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
```

```{r}
x_train[[1]][1:12]
```

```{r}
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
  word <- names(word_index)
  idx <- unlist(word_index, use.names = FALSE)
  word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
  idx <- c(0:3, idx + 3)
  words <-word[match(text, idx, 2)]
  paste(words, collapse = " ")
}

decode_review(x_train[[1]][1:12], word_index)
```

```{r}
library(Matrix)
one_hot <- function(sequences, dimension) {
  seqlen <- sapply(sequences, length)
  n <- length(seqlen)
  rowind <- rep(1:n, seqlen)
  colind <- unlist(sequences)
  sparseMatrix(i = rowind, j = colind,
               dims = c(n, dimension))
}
```

```{r}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)

nnzero(x_train_1h) / (25000 * 10000)
```

```{r}
set.seed(3)
ival <- sample(seq(along = y_train), 2000)
ival <- sort(ival)
```

```{r}
library(glmnet)
fitlm <- glmnet(x_train_1h[-ival, ], y_train[-ival],
                family = 'binomial', standardize=TRUE)
classlmv <- predict(fitlm, x_train_1h[ival, ]) > 0
acclmv <- apply(classlmv, 2, accuracy, y_train[ival] > 0)
```

```{r}
par(mar = c(4, 4, 4, 4), mfrow = c(1,1))
plot(-log(fitlm$lambda), acclmv)
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu",
input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model %>% compile(optimizer = "rmsprop",
loss = "binary_crossentropy", metrics = c("accuracy"))
```

```{r}
history <- model %>% fit(x_train_1h[-ival, ], y_train[-ival],
epochs = 20, batch_size = 512,
validation_data = list(x_train_1h[ival, ], y_train[ival]))
```

```{r}
history <- model %>% fit(
x_train_1h[-ival, ], y_train[-ival], epochs = 20, batch_size = 512, validation_data = list(x_test_1h, y_test)
)
```

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

