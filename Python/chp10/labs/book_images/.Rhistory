with(Gitters[testid, ], mean(abs(lpred - Salary))))
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
reticulate::repl_python()
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
lfit
?lm
testid
summary(lfit)
testid
sort(testid_
sort(testid)
sort(testid)
Gitters[-testid]
nrows(Gitters[-testid])
nrows(Gitters[-testid]))
nrow(Gitters[-testid])
nrow(Gitters[-testid,])
testid
testid.sort
sort(test_id)
sort(testids)
sort(testid)
length(testid)
len(Gitters[-testid, ])
length(Gitters[-testid, ])
nrows(Gitters[-testid, ])
nrow(Gitters[-testid, ])
Gitters
dim(Gitters)
summary(lfit)
?with
x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))
x
?scale
?model.matrix
x
x[-testid]
model.matrix(Salary ~. -1, data=Gitters)
x[-testid]
model.matrix(Salary ~. -1, data=Gitters)
x[-testid]
x <- model.matrix(Salary ~ . - 1, data = Gitters)
x_scale <- scale(x)
x
x[-testid]
x_scale[-testid]
x_scale
x_scale[-testid]
library(glmnet)
cvfit <- cv.glmnet(x[-testid, ], y[-testid],
type.measure = "mae")
x <- model.matrix(Salary ~ . - 1, data = Gitters)
x_scale <- scale(x)
y <- Gitters$Salary
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lamnda.min")
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
?cv.glmnet
x[-testid]
View(x)
View(x)
View(Gitters)
x
View(x_scale)
cpred
?cv.glmnet
View(cpred)
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae", alpha=0)
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
?predict
cvfit
summary(cvfit)
cvfit$lambda
cvfit$index
?cv.glmnet
cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
library(keras)
?layer_dense
?compile
library(keras)
cvfit <- cv.glmnet(x[-testid, ], y[-testid], type.measure = 'mae')
cpred <- predict(cvfit, x[testid, ], s='lambda.min')
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(keras)
modnn <- keras_model_sequential() %>%
layer_dense(units = 50, activation = 'relu',
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1)
x <- model.matrix(Salary ~ . -1, data = Gitters) %>% scale()
modnn %>% compile(loss = 'mse',
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- modnn %>% fit(
x[-testid, ], y[-testid], epochs=1500, batch_size=32,
validation_data = list(x[testid, ], y[testid])
)
plot(history)
history$metrics
history$metrics$loss
plot(history)
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, n)
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, n)
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
x <- model.matrix(Salary ~ . - 1, data = Gitters)
x_scale <- scale(x)
y <- Gitters$Salary
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, n)
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, n)
View(Gitters)
lfit <- lm(Salary ~ ., data=Gitters[-testid, ])
lfit <- lm(Salary âˆ¼ ., data = Gitters[-testid, ])
lfit <- lm(Salary ~ ., data = Gitters[-testid, ])
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
lfit <- lm(Salary ~ ., data = Gitters[-testid, ])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
x <- model.matrix(Salary ~ . - 1, data = Gitters)
x_scale <- scale(x)
y <- Gitters$Salary
library(glmnet)
cvfit <- cv.glmnet(x_scale[-testid, ], y[-testid],
type.measure = "mae")
cpred <- predict(cvfit, x_scale[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
library(keras)
modnn <- keras_model_sequential() %>%
layer_dense(units = 50, activation = 'relu',
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1)
x <- model.matrix(Salary ~ . -1, data = Gitters) %>% scale()
modnn %>% compile(loss = 'mse',
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- modnn %>% fit(
x[-testid, ], y[-testid], epochs=1500, batch_size=32,
validation_data = list(x[testid, ], y[testid])
)
plot(history)
npred <- predict(modnn, x[testid, ])
mean(abs(y[testid] - npred))
npred
update.packages(ggplot)
update.packages(ggplot2)
install.packages(ggplot2)
update.packages('ggplot2')
library(ggplot2)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
dim(x_train)
dim(x_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_train
View(x_train)
dim(x_train)
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
mnist <- dataset_mnist()
x_train <- mnist$train$x
g_train <- mnist$train$y
x_test <- mnist$test$x
g_test <- mnist$test$y
dim(x_train)
dim(x_test)
y_train <- to_categorical(g_train, 10)
y_train
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
y_test
x_train <- x_train / 255
x_test <- x_test / 255
modelnn <- keras_model_sequential()
modelnn %>%
layer_dense(units=256, activation='relu', input_shape= c(784)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units=128, activation='relu') %>%
layer_dropout(rate=0.3) %>%
layer_dense(units=10, activation='softmax')
summary(modelnn)
modelnn %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy')
)
system.time(
history <- modelnn %>%
fit(x_Train, y_Train, epochs = 30, batch_size = 128,
validation_split = 0.2)
)
system.time(
history <- modelnn %>%
fit(x_train, y_train, epochs = 30, batch_size = 128,
validation_split = 0.2)
)
system.time(
history <- modelnn %>%
fit(x_train, y_train, epochs = 30, batch_size = 128,
validation_split = 0.2)
)
history <- modelnn %>%
fit(x_train, y_train, epochs = 30, batch_size = 128,
validation_split = 0.2)
dim(x_train)
mnist <- dataset_mnist()
x_train <- mnist$train$x
g_train <- mnist$train$y
x_test <- mnist$test$x
g_test <- mnist$test$y
dim(x_train)
dim(x_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
x_train <- x_train / 255
x_test <- x_test / 255
modelnn <- keras_model_sequential()
modelnn %>%
layer_dense(units=256, activation='relu', input_shape= c(784)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units=128, activation='relu') %>%
layer_dropout(rate=0.3) %>%
layer_dense(units=10, activation='softmax')
summary(modelnn)
modelnn %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy')
)
system.time(
history <- modelnn %>%
fit(x_train, y_train, epochs = 30, batch_size = 128,
validation_split = 0.2)
)
plot(history, smooth=FALSE)
?drop
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict_classes(x_test) %>% accuracy(g_test)
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict(x_test) %>% k_argmax
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict(x_test) %>% k_argmax()
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict(x_test) %>% accuracy(g_test)
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict(x_test) %>% accuracy(g_test)
accuracy <- function(pred, truth)
mean(drop(pred) == drop(truth))
modelnn %>% predict_classes(x_test) %>% accuracy(g_test)
modellr <- keras_model_sequential() %>%
layer_dense(input_shape = 784, units = 10,
activation='softmax')
summary(modellr)
modellr %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
modellr %>% fit(x_train, y_train, epochs = 30,
batch_size = 128, validation_split = 0.2)
modellr %>% predict_classes(x_test) %>% accuracy(g_test)
?range
range(x_train)
range(x_train[1,,, 1])
range(x_train[1,, 1])
range(x_train[1, 1])
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
range(x_train[1,,, 1])
range(x_train[1,, 1])
range(x_train[1,,, 0])
range(x_train[0,,, 0])
type(x_train)
typeof(x_train)
x_train
cifar100 <- dataset_cifar100()
names(cifar100)
dim(x_train)
range(x_train[1,,, 1])
x_train <- x_train / 255
x_test <- x_test / 255
y_test <- to_categorical(g_test, num_classes=100)
library(jpeg)
?par
sample(seq(50000), 25)
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[1,,, ]))
index
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[1,,, ]))
index
?as.raster
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,, ]))
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
padding= 'same', activation = 'relu',
input_shape = c(32, 32, 3)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3),
padding = 'same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3,3),
padding='same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 256, kernel_size = c(3,3),
padding='same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 100, activation = 'softmax')
summary(model)
model %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epcohs = 30,
batch_size = 128, validation_split = 0.2)
model %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epochs = 30,
batch_size = 128, validation_split = 0.2)
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
g_train <- cifar100$train$y
x_test <- cifar100$test$x
g_test <- cifar100$test$y
dim(x_train)
range(x_train[1,,, 1])
x_train <- x_train / 255
x_test <- x_test / 255
y_test <- to_categorical(g_test, num_classes=100)
x_train <- x_train / 255
x_test <- x_test / 255
y_test <- to_categorical(g_test, num_classes=100)
dim(y_train)
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(g_train, num_classes=100)
dim(y_train)
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,, ]))
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
g_train <- cifar100$train$y
x_test <- cifar100$test$x
g_test <- cifar100$test$y
dim(x_train)
range(x_train[1,,, 1])
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(g_train, num_classes=100)
dim(y_train)
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,, ]))
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
padding= 'same', activation = 'relu',
input_shape = c(32, 32, 3)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3),
padding = 'same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3,3),
padding='same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 256, kernel_size = c(3,3),
padding='same', activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 100, activation = 'softmax')
summary(model)
model %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epochs = 30,
batch_size = 128, validation_split = 0.2)
model %>% predict_classes(x_test) %>% accuracy(g_test)
?names
!pwd
pwd
setwd('/Users/rancher/Google Drive/Coding/ISLR2/Python/chp10/labs/book_images')
img_dir <- '/Users/rancher/Google Drive/Coding/ISLR2/Python/chp10/labs/book_images'
image_names <- list.files(img_dir)
image_names
img_dir <- '/Users/rancher/Google Drive/Coding/ISLR2/Python/chp10/labs/book_images'
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
img_path <- paste(img_dir, image_names[i], sep='/')
img <- image_load(img_path, target_size = c(224, 224))
x[i,,, ] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
}
img_dir <- '/Users/rancher/Google Drive/Coding/ISLR2/Python/chp10/labs/book_images'
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
img_path <- paste(img_dir, image_names[i], sep='/')
img <- image_load(img_path, target_size = c(224, 224))
x[i,,, ] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
model <- application_resnet50(weights = 'imagenet')
summary(model)
pred6 <- model %>% predict(x) %>%
imagenet_decode_predictions(top = 3)
names(pred6) <- image_names
print(pred6)
View(pred6)
